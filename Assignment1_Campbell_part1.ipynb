{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# import data processing\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "# importing other packages which may be essential in the future.\n",
    "import os\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/magimac/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/magimac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# impot nltk nd all the important packages\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "correct_spelling = words.words()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = open(\"/Users/magimac/Documents/Advanced Text Mining/moby.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens: 255018\n",
      "---------------------------------------------\n",
      "The total number of unique tokens: 20754\n"
     ]
    }
   ],
   "source": [
    "# Q1 - How many tokens and unique tokens are in the text? \n",
    "\n",
    "tokens = nltk.word_tokenize(moby)\n",
    "total_tokens = \"The total number of tokens: \" + str(len(tokens)) # this should be 255018\n",
    "print(total_tokens)\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "unique = nltk.FreqDist(tokens)\n",
    "unique_tokens = \"The total number of unique tokens: \" + str(len(unique)) # this should be 20754\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens after verb lemmatization: 33440\n",
      "-------------------------------------------------------------------\n",
      "The total number of unique tokens after lemmatization: 5774\n"
     ]
    }
   ],
   "source": [
    "# Q2 - Apply lemmatizations on the verbs in the text, recalculate the number of tokens and unique tokens.\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemma = nltk.pos_tag(nltk_tokens)\n",
    "\n",
    "counts = Counter(tag for word, tag in lemma if tag in('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'))\n",
    "str(counts) # this is the number of tokens for each verb type.\n",
    "number_verb = 5112 + 3517 +8786 + 4858 + 6884 + 4283 # total up all the verbs.\n",
    "print(\"The total number of tokens after verb lemmatization: \" + str(number_verb)) # this shoul be 33440\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "only_verbs = [tag for word, tag in lemma if tag in('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ')] #just verbs alone\n",
    "tagged_verbs = [word for word, tag in lemma if tag in('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ')]\n",
    "unique_verb_tokens = nltk.FreqDist(tagged_verbs)# like what we did in Q1\n",
    "print(\"The total number of unique tokens after lemmatization: \" + str(len(unique_verb_tokens))) # this should be 5774\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'history': 15})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 - What percentage of tokens is ' HISTORY' or 'history'?\n",
    "\n",
    "# creating a function which will filter out lowercase 'history' for us\n",
    "\n",
    "def lower (datalist):\n",
    "    return [val for val in datalist\n",
    "            if re.search(r'^history', val)]\n",
    "history = lower(nltk_tokens)\n",
    "Counter(history)\n",
    "\n",
    "# this should be totaling to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'HISTORY': 4})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a function which will filter out cap locked 'HISTORY' for us\n",
    "\n",
    "def CAPS (datalist):\n",
    "    return [val for val in datalist\n",
    "            if re.search(r'^HISTORY', val)]\n",
    "HISTORY = CAPS(nltk_tokens)\n",
    "Counter(HISTORY)\n",
    "\n",
    "# this should be totaling to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Percentage of \"history\" and \"HISTORY\": 0.007450454477723141%\n"
     ]
    }
   ],
   "source": [
    "percent = ((15+4)/255018)*100\n",
    "\n",
    "print('The total Percentage of \"history\" and \"HISTORY\": ' + str(percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4 - What are the 10 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "most_frequent_tokens = unique.most_common(10)\n",
    "most_frequent_tokens\n",
    "\n",
    "# the top 10 most unique tokens and their frequencies should be listed below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
